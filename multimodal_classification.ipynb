{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"multimodal_classification.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Yc6N_t3IfOhl"},"source":["# Multimodal Classification of sound and pictures\r\n","ATSI Project : Machine Learning\r\n","Group : Pierre-Antoine COMBY & Maxime ZAGAR\r\n","  \r\n","This project aims to create an learning algorithm dedicated to recognize an environment from pictures and sounds. In our case study, we have to differentiate those data in 9 different classes. \r\n","  \r\n","The provided data have the following shape :  \r\n","- Pictures are at the format 256x256\r\n","- Each sound is described by 104 mffc coefficients\r\n","\r\n","**Useful links :**  \r\n","Competition address : https://www.kaggle.com/c/iogs-atsi-multimodal/leaderboard  \r\n","Keras : https://www.kaggle.com/sinkie/keras-data-augmentation-with-multiple-inputs\r\n"]},{"cell_type":"code","metadata":{"id":"onzYZTwOfI3P","colab":{"base_uri":"https://localhost:8080/","height":62},"executionInfo":{"status":"ok","timestamp":1615214136360,"user_tz":-60,"elapsed":34048,"user":{"displayName":"pierre antoine Comby","photoUrl":"","userId":"15891237432058929795"}},"outputId":"a8b8463b-8015-4844-fa71-0b64c7c043fe"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","import tensorflow as tf\n","tf.test.gpu_device_name()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/device:GPU:0'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"FTchj8h5-hPN"},"source":["\r\n","## Dataset pre processing\r\n","\r\n","The data (sound+picture) describe these 9 different locations :\r\n","- Forest\r\n","- An urban place (city)\r\n","- Beach\r\n","- Classroom\r\n","- River\r\n","- Jungle\r\n","- Restaurant\r\n","- Football match\r\n","- Grocery-Store"]},{"cell_type":"code","metadata":{"id":"oT-9J6l5flqJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615214321745,"user_tz":-60,"elapsed":1461,"user":{"displayName":"pierre antoine Comby","photoUrl":"","userId":"15891237432058929795"}},"outputId":"10a5607c-e2c9-4e4a-e486-70a1a2253d79"},"source":["DATA_DIR = 'gdrive/My Drive/Colab_Notebooks/multimodal_classification/data'\n","import matplotlib.pyplot as plt\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os \n","import skimage\n","from skimage import io\n","\n","#!pip install keras-rectified-adam\n","\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","#from keras_radam import RAdam\n","\n","from PIL import Image\n","\n","\n","CLASS_NAME=[ 'FOREST', 'CITY', 'BEACH', 'CLASSROOM', 'RIVER', 'JUNGLE', 'RESTAURANT', 'GROCERY-STORE', 'FOOTBALL-MATCH']\n","train_df = pd.read_csv(os.path.join(DATA_DIR,'data_train.csv'), delimiter=',', nrows=None)\n","\n","\n","print(train_df.head())\n","dataTrain, dataValid = train_test_split(train_df,train_size=.80,random_state=42)\n","\n","test_df = pd.read_csv(os.path.join(DATA_DIR,'data_test_novt.csv'), delimiter=',', nrows = None)\n","\n","print(f\"Number of labelled samples: {len(train_df)}\")\n","print(f\"Number of labelled validation samples: {len(dataValid)}\")\n","print(f\"Number of labelled train samples: {len(dataTrain)}\")\n","print(f\"Number of unlabelled samples {len(test_df)}\")\n","print(f\"Number of classes: {len(CLASS_NAME)}\")\n","\n","# Test dataset, nolabel is provided. \n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["                IMAGE     mfcc_1     mfcc_2  ...   mfcc_103   mfcc_104  CLASS\n","0  trainimg_00000.png  11.112999   4.439105  ... -11.933302 -14.578534      3\n","1  trainimg_00001.png  13.567897  -1.470553  ...  14.197092  -9.513413      0\n","2  trainimg_00002.png  12.585137   1.143402  ...   9.582895   0.731367      3\n","3  trainimg_00003.png  17.783320   4.445305  ...  -5.714941  -6.054448      4\n","4  trainimg_00004.png  12.348299 -11.143099  ...  21.121838   5.865456      5\n","\n","[5 rows x 106 columns]\n","Number of labelled samples: 13802\n","Number of labelled validation samples: 2761\n","Number of labelled train samples: 11041\n","Number of unlabelled samples 3450\n","Number of classes: 9\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NOk0Wm8kfN3I"},"source":["## Audio classification\n","\n","To see what's possible with only  the audio data, let's try a classical MLP. With only  a few thousand of parameters, the classification is already pretty good. The training is also very fast. \n","\n","As we have a "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YpgnAvUzvF3_","executionInfo":{"status":"ok","timestamp":1615214324912,"user_tz":-60,"elapsed":725,"user":{"displayName":"pierre antoine Comby","photoUrl":"","userId":"15891237432058929795"}},"outputId":"ccb9482a-2490-4ab5-f5fd-35506e13225f"},"source":["dataTrainArray = np.array(dataTrain)\n","dataValidArray = np.array(dataValid)\n","audioTrain,yTrain = dataTrainArray[:,1:-1].astype('float32'), dataTrainArray[:,-1].astype('int')\n","audioValid,yValid = dataValidArray[:,1:-1].astype('float32'), dataValidArray[:,-1].astype('int')\n","\n","def create_mlp():\n","    audio_inputs = keras.Input(shape=audioTrain.shape[1])\n","    audio1 = layers.Dense(64,activation=\"relu\")(audio_inputs)\n","    audio2 = layers.Dense(32,activation=\"relu\")(audio1)\n","    audio3 = layers.Dense(16,activation=\"relu\")(audio2)\n","    audio_outputs = layers.Dense(9,activation=\"softmax\")(audio3)\n","    MLP = keras.Model(inputs=audio_inputs, outputs=audio_outputs)\n","    return MLP, audio_inputs, audio_outputs\n","\n","MLP, audio_inputs, audio = create_mlp()\n","MLP.summary()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 104)]             0         \n","_________________________________________________________________\n","dense (Dense)                (None, 64)                6720      \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 32)                2080      \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 16)                528       \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 9)                 153       \n","=================================================================\n","Total params: 9,481\n","Trainable params: 9,481\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4ZAqTLKGy3GO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615214448652,"user_tz":-60,"elapsed":46293,"user":{"displayName":"pierre antoine Comby","photoUrl":"","userId":"15891237432058929795"}},"outputId":"67ca5b81-149c-42db-ed52-caf649d1ce09"},"source":["MLP.compile(\n","     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer=keras.optimizers.RMSprop(),\n","    metrics=[\"accuracy\"],\n",")\n","history = MLP.fit(audioTrain,yTrain,batch_size=16,epochs=50,validation_split=0.2,verbose=0)\n","\n","test_scores = MLP.evaluate(audioValid, yValid)\n","print(\"Loss\",test_scores[0])\n","print(\"accuracy:\", test_scores[1])"],"execution_count":8,"outputs":[{"output_type":"stream","text":["87/87 [==============================] - 0s 2ms/step - loss: 2.2115 - accuracy: 0.8950\n","Loss 2.211484670639038\n","accuracy: 0.8949655890464783\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7ga4ZmJ_SLMI"},"source":["The training is fast, and the accuracy is already relevant. "]},{"cell_type":"markdown","metadata":{"id":"1OyyGIZAUlfF"},"source":["## Image Classification\n","\n","In the same way, one can classify using only the image information , which is a classic ML problem. The following CNN, inspired by AlexNet and VGG famous network, provides a relative good accuracy, but is however very slow to train (due to a increase number of parameters)."]},{"cell_type":"code","metadata":{"id":"8EKYxhHCUgPQ"},"source":["def small_cnn(input_shape):\n","    params = {'activation':'relu','kernel_initializer':'he_uniform', 'padding':'same'}\n","    X_input = keras.Input(input_shape)\n","    X = layers.AveragePooling2D((4,4))(X_input) # downscale to 64x64\n","\n","    X = layers.Conv2D(32,(3,3), **params, input_shape=(64,64,3))(X)\n","    X = layers.BatchNormalization()(X)   \n","    X = layers.Conv2D(32,(3,3), **params)(X)\n","    X = layers.BatchNormalization()(X)   \n","    X = layers.MaxPooling2D((2,2))(X)\n","    X = layers.Dropout(0.2)(X)\n","\n","    X = layers.Conv2D(64,(3,3), **params, input_shape=(64,64,3))(X)\n","    X = layers.BatchNormalization()(X)   \n","    X = layers.Conv2D(64,(3,3), **params)(X)\n","    X = layers.BatchNormalization()(X)   \n","    X = layers.MaxPooling2D((2,2))(X)\n","    X = layers.Dropout(0.3)(X)\n","\n","    X = layers.Conv2D(128,(3,3), **params, input_shape=(64,64,3))(X)\n","    X = layers.BatchNormalization()(X)   \n","    X = layers.Conv2D(128,(3,3), **params)(X)\n","    X = layers.BatchNormalization()(X)   \n","    X = layers.MaxPooling2D((2,2))(X)\n","    X = layers.Dropout(0.4)(X)\n","    \n","    X = layers.Flatten()(X)\n","    X = layers.Dense(128, activation = 'relu',kernel_initializer='he_uniform')(X)\n","    X = layers.BatchNormalization()(X)   \n","    X = layers.Dropout(0.5)(X)\n","    X = layers.Dense(9,activation='softmax')(X)\n","    model = keras.Model(inputs = X_input, outputs = X, name='smallCNN')\n","    return model,X_input,X"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YopQ0jOHpZOP"},"source":["## Joint Classification\n","To improve the Classification, we have to use the complete information at our disposal, here the image joint with the audio data. \n","\n","\n","### Custom DataGeneration\n","\n","The dataset of image is relatively small, and a closer look shows that it is in fact built from a few different videos. \n","\n","To provides the training data together (image file, sound, label) to the model, it is required to create a DataGenerator, this generator is also used to create some augmentations on images (small rotations, translations, zooms and flip), with the aim of making the image classification more robust. \n","No transformation is applied to the sound data, as no trivial augmentation is possible with mfcc coefficients.\n","\n","\n","The same is done for the test data, only providing sound and image, with no extra transformations. "]},{"cell_type":"code","metadata":{"id":"GpNUiXRyokS2"},"source":["class CustomDataGenerator(keras.utils.Sequence):\n","  def __init__(self,dataframe,batch_size=32,shuffle=True,augment=False):\n","    self.batch_size=batch_size\n","    self.df = dataframe\n","    self.indices = self.df.index.tolist()\n","    self.shuffle = shuffle\n","    if augment:\n","     self.augmentor = ImageDataGenerator(\n","        rescale=1./255,\n","        rotation_range=10,\n","        width_shift_range=0.1,\n","        height_shift_range=0.1,\n","        zoom_range=0.1,\n","        horizontal_flip=True,\n","        fill_mode='nearest')\n","    else:\n","      self.augmentor = ImageDataGenerator(rescale=1./255)\n","    self.on_epoch_end()\n","\n","  def on_epoch_end(self):\n","    self.index = np.arange(len(self.indices))\n","    if self.shuffle:\n","      np.random.seed(42)\n","      np.random.shuffle(self.index)\n","  \n","  def __len__(self):\n","    return len(self.indices)//self.batch_size\n","  \n","  def __getitem__(self,index):\n","    index = self.index[index*self.batch_size:(index+1)*self.batch_size]\n","    batch_indices = [self.indices[k] for k in index]\n","    X,y = self.__data_generation(batch_indices)\n","    return X,y\n","\n","  def __data_generation(self,batch_indices):\n","    Ximage = np.empty((self.batch_size,256,256,3),dtype='float32')\n","    Xaudio = np.empty((self.batch_size, 104),dtype='float32')\n","    y = np.empty(self.batch_size,dtype=int)\n","\n","    for i, idx in enumerate(batch_indices):\n","      Ximage[i,:] = keras.preprocessing.image.load_img(os.path.join(DATA_DIR,self.df.at[idx,'IMAGE']))\n","      Xaudio[i,:] = np.array(self.df.loc[idx])[1:-1]\n","      y[i] = np.array(self.df.at[idx,'CLASS'])\n","    X_gen = self.augmentor.flow([Ximage,Xaudio], y)\n","    return next(X_gen)\n","\n","class CustomTestDataGenerator(keras.utils.Sequence):\n","  def __init__(self,dataframe,batch_size=1):\n","    self.batch_size=batch_size\n","    self.df = dataframe\n","    self.indices = self.df.index.tolist()\n","    self.index = np.arange(len(self.indices))\n","    self.augmentor = ImageDataGenerator(rescale=1./255)\n","  def __len__(self):\n","    return len(self.indices)//self.batch_size\n","  \n","  def __getitem__(self,index):\n","    index = self.index[index*self.batch_size:(index+1)*self.batch_size]\n","    batch_indices = [self.indices[k] for k in index]\n","    X = self.__data_generation(batch_indices)\n","    return X\n","\n","  def __data_generation(self,batch_indices):\n","    Ximage = np.empty((self.batch_size,256,256,3),dtype='float32')\n","    Xaudio = np.empty((self.batch_size, 104),dtype='float32')\n","    for i, idx in enumerate(batch_indices):\n","      Ximage[i,:] = keras.preprocessing.image.load_img(os.path.join(DATA_DIR,self.df.at[idx,'IMAGE']))\n","      Xaudio[i,:] = np.array(self.df.loc[idx])[1:]\n","    X_gen = self.augmentor.flow([Ximage,Xaudio])\n","    return next(X_gen)\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sjUJ_6eXQQJy"},"source":["## Split Network\n","\n","A first approach to train is to use to parallel network (possibly trained separatly) and combine their decision output."]},{"cell_type":"code","metadata":{"id":"WMBuIz10swML"},"source":["Xgen = CustomDataGenerator(dataTrain,augment=True)\n","XValid=  CustomDataGenerator(dataValid,augment=False)\n","\n","def multimodal_network():\n","  MLP, audio_input, audio_output = create_mlp()\n","  #CNN, image_input, image_output = AlexNet((256,256,3))\n","  CNN, image_input, image_output = small_cnn((256,256,3))\n","\n","  combined = layers.concatenate([audio_output, image_output])\n","  z = layers.Dense(15,activation='relu')(combined)\n","  output = layers.Dense(9,activation='softmax')(z)\n","\n","  return keras.Model(inputs=[image_input, audio_input],outputs=output)\n","\n","MLM = multimodal_network()\n","MLM.summary()\n","keras.utils.plot_model(MLM, show_shapes=True, show_layer_names=True, to_file='model.png')\n","\n","from IPython.display import Image\n","Image(retina=True, filename='model.png')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iQlb5S0KKYaf"},"source":["MLM.compile(\n","        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","  optimizer= keras.optimizers.Adam(),\n","  metrics=[\"accuracy\"],\n",")\n","history = MLM.fit(Xgen,epochs=50,validation_data=XValid,workers=2,verbose=0)\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.legend('accuracy','validation_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mnd0genq4x_4","executionInfo":{"status":"ok","timestamp":1614272975637,"user_tz":-60,"elapsed":18583,"user":{"displayName":"pierre antoine Comby","photoUrl":"","userId":"15891237432058929795"}},"outputId":"656ab0d8-135b-4778-add6-5370f3eb671f"},"source":["scores = MLM.evaluate(XValid)\n","print(scores)\n","MLM.save(os.path.join(DATA_DIR,'multi_mod_class'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["86/86 [==============================] - 14s 161ms/step - loss: 0.0457 - accuracy: 0.9876\n","[0.045679979026317596, 0.9876453280448914]\n","INFO:tensorflow:Assets written to: gdrive/My Drive/Colab_Notebooks/multimodal_classification/data/multi_mod_class/assets\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eqPVL73EVBb5"},"source":["## A better Approach\n","\n","The previous network was the simplest merge of two independant network. In particular, the last classifying layers of each indepedant network were used(9+9 -> 15 -> 9) this caused a lot of compression, with potentially a loss of information, which could be usefull for the classification. \n","\n","In the following NN, the last independant layers are not classifying anymore. Furthermore, Drop-out layers with increasing rates have been added to make the network more robust to the data presented. "]},{"cell_type":"code","metadata":{"id":"5MV-IUebNYh2"},"source":["def direct_mix():\n","    image_input_shape=(256,256,3)\n","    audio_input_shape=104\n","\n","    params = {'activation':'relu','kernel_initializer':'he_uniform', 'padding':'same'}\n","    X_image_input = keras.Input(image_input_shape)\n","    X = layers.AveragePooling2D((4,4))(X_image_input) # downscale to 64x64\n","\n","    X = layers.Conv2D(32,(3,3), **params, input_shape=(64,64,3))(X)\n","    X = layers.BatchNormalization()(X)   \n","    X = layers.Conv2D(32,(3,3), **params)(X)\n","    X = layers.BatchNormalization()(X)   \n","    X = layers.MaxPooling2D((2,2))(X)\n","    X = layers.Dropout(0.2)(X)\n","\n","    X = layers.Conv2D(64,(3,3), **params, input_shape=(64,64,3))(X)\n","    X = layers.BatchNormalization()(X)   \n","    X = layers.Conv2D(64,(3,3), **params)(X)\n","    X = layers.BatchNormalization()(X)   \n","    X = layers.MaxPooling2D((2,2))(X)\n","    X = layers.Dropout(0.3)(X)\n","\n","    X = layers.Conv2D(128,(3,3), **params, input_shape=(64,64,3))(X)\n","    X = layers.BatchNormalization()(X)   \n","    X = layers.Conv2D(128,(3,3), **params)(X)\n","    X = layers.BatchNormalization()(X)   \n","    X = layers.MaxPooling2D((2,2))(X)\n","    X = layers.Dropout(0.4)(X)\n","    \n","    X = layers.Flatten()(X)\n","    X = layers.Dense(128, activation = 'relu',kernel_initializer='he_uniform')(X)\n","    X = layers.BatchNormalization()(X)  \n","    X = layers.Dropout(0.5)(X)\n","    # audio net\n","    X_audio_input = keras.Input(audio_input_shape)\n","    X1 = layers.Dense(256,activation = 'relu',kernel_initializer='he_uniform')(X_audio_input)\n","    X1 = layers.Dense(128,activation = 'relu',kernel_initializer='he_uniform')(X1)\n","\n","    X = layers.concatenate([X, X1]) # 256 final parameters\n","    output = layers.Dense(9,activation=\"softmax\")(X)\n","\n","    return keras.Model(inputs=[X_image_input, X_audio_input],outputs=output) \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EhnSZYeEWW_h"},"source":["### Training\n","\n","The training has been realised only on Google Colab GPU (Nivdia K100), and the lack of computing power has limited the search for optimal meta parameters (number of Epoch, data augmentation variations, etc...). \n","\n","The use of data augmentation has not shown any particular benefit with this dataset, and has thus been disable. We assume that this mostly due to the nature of the validation and test data, both extracted from the same handfull of (relatively stabled) videos. In a more heteregeneous dataset (or with more class ) it may be reused. "]},{"cell_type":"code","metadata":{"id":"UOAemZVeVLu8"},"source":["\n","XgenFull = CustomDataGenerator(train_df,augment=False)\n","Xgen = CustomDataGenerator(dataTrain,augment=False)\n","XValid=  CustomDataGenerator(dataValid,augment=False)\n","XTest =  CustomTestDataGenerator(test_df)\n","\n","directMix = direct_mix()\n","directMix.compile(\n","        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","  optimizer= keras.optimizers.Adam(),\n","  metrics=[\"accuracy\"],\n",")\n","\n","checkpoint_filepath = 'gdrive/My Drive/Colab_Notebooks/best_weight/'\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    save_weights_only=True,\n","    monitor='val_accuracy',\n","    mode='max',\n","    save_best_only=True)\n","# save the best model\n","history = directMix.fit(Xgen,epochs=50,validation_data=XValid,workers=2,verbose=1,callbacks=[model_checkpoint_callback])\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.legend('accuracy','validation_accuracy')\n","directMix.load_weights(checkpoint_filepath)\n","\n","print(directMix.evaluate(XValid))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q1HpOZ_CXiCi"},"source":["## Use of Full data training"]},{"cell_type":"code","metadata":{"id":"4wnAjICWaE68"},"source":["\n","XgenFull = CustomDataGenerator(train_df,augment=False)\n","XTest =  CustomTestDataGenerator(test_df)\n","\n","directMix = direct_mix()\n","directMix.compile(\n","        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","  optimizer= keras.optimizers.Adam(),\n","  metrics=[\"accuracy\"],\n",")\n","\n","checkpoint_filepath = 'gdrive/My Drive/Colab_Notebooks/best_weight_full/'\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    save_weights_only=True,\n","    monitor='accuracy',\n","    mode='max',\n","    save_best_only=True)\n","# save the best model\n","history = directMix.fit(XgenFull,epochs=50,workers=2,verbose=1,callbacks=[model_checkpoint_callback])\n","plt.plot(history.history['accuracy'])\n","plt.legend('accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qDUnG02FXagY"},"source":["## Prediction on Test data and Submission"]},{"cell_type":"code","metadata":{"id":"60DoIvyG68bG"},"source":["XTest =  CustomTestDataGenerator(test_df)\n","directMix.load_weights(checkpoint_filepath)\n","\n","testPred = directMix.predict(XTest)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0L8nAHAnCGct"},"source":["y_classes = testPred.argmax(axis=-1)\n","with open(os.path.join(DATA_DIR,'test.npy'), 'wb') as f:\n","  np.save(f,y_classes)\n","\n","import csv\n","header = ['id','CLASS']\n","with open(os.path.join(DATA_DIR,'out.csv'), 'w') as fh:\n","    writer = csv.writer(fh, delimiter=',')\n","    writer.writerow(h for h in header)\n","    writer.writerows(enumerate(y_classes))"],"execution_count":null,"outputs":[]}]}